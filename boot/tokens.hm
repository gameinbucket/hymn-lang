import "hashset" (hashset, hashset_item)

-- parsing & compiling imports from abritrary directories and files
-- parsing is completely finished before any compiling is done
-- this means we can represent any given directory/module as a generic UID e.g a number 0, 1, 2, 3, etc
-- only at the compilation phase must we convert this UID to actual unique directory/modules
-- the compilation phase can determine path & naming conflicts and generate an ideally replicable unique name-space

-- example: 
--      --> import "hashmap" (hashmap)
--      --> import "library" (library)
--      --> x = hashmap<string,library>
--      --> hashmap => /user/code/project/hashmap.hm => 0
--      --> library => /user/code/modules/library.hm => 1
--      --> %0.hashmap<string,%1.library>

new_hashset() hashset<string>
    return hashset(
        capacity: 12
        get_code: hashset.string_hashcode
        table: [12:]maybe<hashset_item<string>>)

const words = new_hashset()
const primitives = new_hashset()

init()
    words.add("import")
    words.add("macro")

    primitives.add("int")
    primitives.add("float")

digit(c char) bool
    match c
        '0' | '1' | '2' | '3' |
        '4' | '5' | '6' | '7' |
        '8' | '9' => return true
        _ => pass
    return false

class token
    depth int
    name  string
    value string

token_for(depth int, name string) token
    return token(depth, name, "")

token to_string() string
    mutable s = "{depth:" + to_str(self.depth) + ", type:" + self.name
    if self.value != ""
        s += ", value:" + self.value
    s += "}"
    return s

class tokenizer
    tokens []token
    depth  int
    eof    token

new_tokenizer() tokenizer
    return tokenizer()

tokenizer simple_token(name string) token
    return token_for(self.depth, name)

tokenizer get(pos int) token
    if pos < len(self.tokens)
        return self.tokens[pos]
    return self.eof
