import "tokens" (token)
import "stream" (stream)

class tokenizer
    file_stream  stream
    current      string
    tokens       []token
    eof          token
    size         int
    depth        int
    update_depth bool

def new_token(depth int, name string) token
    return token(depth, name, "")

def new_tokenizer(file_stream stream) tokenizer
    return tokenizer (
        file_stream: file_stream
        eof: new_token(0, "eof")
        update_depth: true
        size: len(file_stream.data)
    )

tokenizer new_simple_token(name string) token
    return new_token(self.depth, name)

tokenizer get(pos int) token
    if pos < len(self.tokens)
        return self.tokens[pos]
    return self.eof
