import tokens (token)
import stream (stream)

class tokenizer
    file_stream  stream
    current      string
    tokens       []token
    eof          token
    size         int
    depth        int
    update_depth bool

def new(file_stream stream) tokenizer
    return tokenizer (
        file_stream: file_stream
        eof: tokens.new(0, "eof")
        update_depth: true
        size: len(file_stream.data)
    )

def tokenizer.new_simple_token(name string) token
    return tokens.new(self.depth, name)

def tokenizer.get(pos int) token
    if pos < len(self.tokens)
        return self.tokens[pos]
    return self.eof
